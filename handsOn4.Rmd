---
title: "Hands On 4: Information Retrieval and Text Mining"
output: html_notebook
---
### For R beginners
New chunk *Ctrl+Alt+I*

Execute chunk *Ctrl+Shift+Enter*

Execute all chunks *Ctrl+Alt+R*

HTML preview *Ctrl+Shift+K*

# 4.1 Information Retrieval

```{r}
library(tm)
library(text2vec)
library(textstem)
library(dplyr)
library(tidyr)
library(ggplot2)
```

1. Using the functions VectorSource() and Corpus(), start by creating a corpus with three “documents” containing the following text:
  • “Mining is important for finding gold”
  • “Classification and regression are data mining”
  • “Data mining deals with data”
Then, use the function DocumentTermMatrix() to represent the documents.

```{r}
# Preparation
docs <- c("Mining is important for finding gold", "Classification and regression are data mining", "Data mining deals with data")
vc <- VectorSource(docs)
corpus <- Corpus(vc)
dtm <- DocumentTermMatrix(corpus)
print(dtm)
```


```{r}
# a) Use the functions nDocs(), nTerms(), Terms() to get some infomation on the DocumentTermMatrix you have created.
nDocs(dtm)
nTerms(dtm)
Terms(dtm)
```

```{r}
# b) If you inspect the DocumentTermMatrix, what information does it give you? What is the document representation model employed by default? If you want to get the complete DocumentTermMatrix, you should use the function as.matrix
inspect(dtm)
# full display of terms use
as.matrix(dtm)
dtm.tf <- dtm
```

```{r}
# c) Use the function weightBin() on the original document term matrix to represent the documents with a vector space model, but with a binary scheme.
dtm.bin <- weightBin(dtm)
inspect(dtm.bin)
```

```{r}
# d) Use the function weightTfIdf() on the original document term matrix to represent the documents with a vector space model, but with TF-IDF scheme.
dtm.tfidf <- weightTfIdf(dtm)
inspect(dtm.tfidf)
as.matrix(dtm.tfidf)
```

```{r}
# e) Did any of the terms get a zero value for all documents? Which ones? What does this tell you about the discriminative power of the term?
# The term mining appears to have value zero for all documents, since it appears in all documents it finds it useless(it has zero discriptive power).
```

```{r}
# f) Analyze the cosine similarity between the three documents, in each weighting scheme. You can use the function sim2 from the package text2vec on each matrix.
sim2(as.matrix(dtm.bin), method = "cosine")
sim2(as.matrix(dtm.tf), method = "cosine")
sim2(as.matrix(dtm.tfidf), method = "cosine")
```

2. Rank the above documents given the query “data mining” by the cosine similarity of the query to each document:

```{r}
# Preparations
cq <- Corpus(VectorSource("data mining"))
dtmq <- DocumentTermMatrix(cq)

# creation of a matrix with 1 row and all terms in use in the previous existent corpus
mq <- matrix(0, ncol=nTerms(dtm.tf),
             dimnames = list("q", Terms(dtm.tf)))

# setting the only row of the query to match the terms in the query
mq[1,Terms(dtmq)] <- 1
mq

# Computing the distances
```

```{r}
# a) using binary scheme
sim2(as.matrix(dtm.bin), mq, method = "cosine")
```

```{r}
# b) using TF scheme
sim2(as.matrix(dtm.tf), mq, method = "cosine")
```

```{r}
# c) using TF-IDF scheme
sim2(as.matrix(dtm.tfidf), mq, method = "cosine")
```

# 4.2 Text Mining
## Processing steps

3. Let us now use a set of documents which represent news from Reuters news agency, related with crude oil. These documents are available on the tm package and are stored as XML files following the format used by Reuters.

```{r}
# example of lemmatization and stemming 
vector <- c("run", "ran", "running", "stayed", "unconsciously", "betrayed")
lemmatize_words(vector)
stem_words(vector)
```

```{r}
# a) Load the above referred files by executing the following code:
# Access sub directory of tm package where the files are stored
reut <- system.file("texts", "crude", package = "tm")
# Creating a corpus using the files from the directory
reuters <- VCorpus(DirSource(reut),
                    readerControl = list(reader = readReut21578XMLasPlain))

reuters
r0 <- reuters
```

```{r}
# b) Inspect the first text of the loaded corpus
inspect(reuters[[1]])
meta(reuters[[1]])
```

```{r}
# c) Load the package wordcloud to obtain a graphical representation of the terms in the corpus.
library(wordcloud)
wordcloud(reuters, colors = rainbow(20))
```

```{r}
# d) Use the function tm_map to apply the following transformations to the texts forming a corpus:
# • strip white space
# • convert everything to lowercase
# • remove english stopwords
# • obtain words stem (keeping only the “root” of each word)
# remove punctuation, by taking into account that intra-words contractions and intra-words dashes should be preserved.

reuters <- tm_map(reuters, stripWhitespace)

reuters <- tm_map(reuters, 
                  content_transformer(tolower))

reuters <- tm_map(reuters, 
                  removeWords,
                  stopwords("english"))

reuters <- tm_map(reuters, stemDocument)

reuters <- tm_map(reuters, removePunctuation,
                  preserve_intra_word_contractions = TRUE,
                  preserve_intra_word_dashes = TRUE)
```

```{r}
# e) Obtain a graphical representation of the frequencies of terms in the transformed corpus. Is it too different from the original representation?
inspect(reuters[[1]])

wordcloud(reuters, colors = rainbow(20))

# wordcloud for just one text
wordcloud(reuters[[2]], colors = rainbow(20))
```

# For the sake of curiosity lets compare 2 transfromations

```{r}
# Stemming
r1 <- tm_map(r0, stemDocument)
wordcloud(r1, colors = rainbow(20))

# Lemmanization
r2 <- tm_map(r0, lemmatize_words)
wordcloud(r2, colors = rainbow(20))
```

```{r}
# f) Convert the transformed corpus into a Document Term Matrix and inspect a few entries of the matrix.
dtm <- DocumentTermMatrix(reuters)

inspect(dtm[5:10, 740:743])
```

```{r}
# g) Use the function FindFreqTerms for obtaining the terms that occur more than 10 times.
findFreqTerms(dtm, 10)
```

```{r}
# h) Use the function findAssocs for obtaining the terms with a correlation higher than 0.8 with the term “opec”, which stands for “Organization of the Petroleum Exporting Countries”.
# correlation means co-ocurrence
findAssocs(dtm, "opec", 0.8)
```

# Document Clustering

# 4. Consider the collection of 50 news articles from the Reuters agency related to corporate acquisitions. This data set comes with package tm and it is already packaged as a Corpus

```{r}
# a) Load and inspect the corpus acq
data(acq)
inspect(head(acq))
```

```{r}
# b) Define a function cleanup() that, given a corpus, transforms every letters to lower case, removes numbers, removes stopwords in English, removes punctuation and strips white spaces. Apply the defined function to the acq corpus. For this corpus remove also the extra word "said".

cleanup <- function(docs, spec.words=NULL){
  # lowercase
  docs <- tm_map(docs, content_transformer(tolower))
  # rm numbers
  docs <- tm_map(docs, removeNumbers)
  # rm english common stopWords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  # if stopwords are specified as a character vector
  if(!is.null(spec.words))
    docs <- tm_map(docs, removeWords, spec.words)
  # rm punctuations
  docs <- tm_map(docs, removePunctuation)
  # rm extra white spaces
  docs <- tm_map(docs, stripWhitespace)
  # text stemming
  docs <- tm_map(docs, stemDocument)
  
  docs
}

wordcloud(acq)
```

```{r}
acq1 <- cleanup(acq, c("said", "the", "and"))

wordcloud(acq1)
```

```{r}
# c) Transform the collection of documents into a Document Term Matrix
dtm <- DocumentTermMatrix(acq1)
dtm
```

```{r}
# d) 
mdf <- as_tibble(as.matrix(dtm))

mdf.freq <- mdf %>%
  select(findFreqTerms(dtm, nDocs(dtm)/2)) %>%
  summarise_all(sum) %>%
  gather() %>%
  arrange(desc(value))

  mdf.freq$key <- 
    factor(mdf.freq$key,
           levels = mdf.freq$key[order(mdf.freq$value)])
  
  ggplot(mdf.freq, aes(x=key, y=value)) +
    geom_bar(stat="identity") +
    labs(x="terms", y="freq") + coord_flip()
  
  freq = data.frame(sort(colSums(as.matrix(dtm)),
                         decreasing = TRUE))
  
  wordcloud(rownames(freq), freq[,1],
            max.words = 50, colors = brewer.pal(3, "Dark2"))
  
```

```{r}
# e)
DistM <- dist(as.matrix(dtm))
Tree <- hclust(DistM)
plot(Tree)
```

```{r}
# f)
```

```{r}
# g)
h <- hclust(DistM, method = "ward.D")
plot(h)

# h)
ClustKey <- cutree(h, 3)
ClustKey
rect.hclust(h,3)
```

```{r}
# i)
c1 <- dtm[ClustKey==1,]
c2 <- dtm[ClustKey==2,]
c3 <- dtm[ClustKey==3,]

plot.wordcloud <- function(dtmc) {
  mdf.c <- as_tibble(as.matrix(dtmc)) %>%
    summarise_all(sum) %>%
    gather() %>%
    arrange(desc(value))
    wordcloud(mdf.c$key, mdf.c$value, min.freq = 5)
}

par(mfrow=c(1,3))
plot.wordcloud(c1)
plot.wordcloud(c2)
plot.wordcloud(c3)
par(mfrow=c(1,1))
```

# 4.2.5

```{r}
data(crude)
data(acq)
docs <- c(acq, crude)
```

```{r}
# a)
cleanup <- function(docs, spec.words=NULL){
  # lowercase
  docs <- tm_map(docs, content_transformer(tolower))
  # rm numbers
  docs <- tm_map(docs, removeNumbers)
  # rm english common stopWords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  # if stopwords are specified as a character vector
  if(!is.null(spec.words))
    docs <- tm_map(docs, removeWords, spec.words)
  # rm punctuations
  docs <- tm_map(docs, removePunctuation)
  # rm extra white spaces
  docs <- tm_map(docs, stripWhitespace)
  # text stemming
  docs <- tm_map(docs, stemDocument)
  
  docs
}

docs <- cleanup(docs, c("said", "reuters", "reuter"))
```

```{r}
# b)
dtm <- DocumentTermMatrix(docs)
```

```{r}
# c)
dtm <- removeSparseTerms(dtm, 0.8)
```

```{r}
# d)
dat <- cbind(data.frame(as.matrix(dtm),
                        class=as.factor(c(rep("acq", 50), rep("crude", 20)))))

dat
```

```{r}
# e)
library(e1071)
library(performanceEstimation)

exp <- performanceEstimation(
  PredTask(class ~., dat),
  c(workflow(learner="naiveBayes"),
    workflowVariants(learner="svm",
                     learner.pars=
                       list(kernel=c("linear",
                                     "radial")))),
  EstimationTask(metrics="err", method=CV())
)
```

```{r}
# f)
summary(exp)

plot(exp)
```

